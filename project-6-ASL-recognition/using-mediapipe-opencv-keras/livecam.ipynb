{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DEEP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using two popular live video processing libraries known as \n",
    "Mediapipe and Open-CV, we can take webcam input and run our \n",
    "previously developed model on real time video stream.\n",
    "'''\n",
    "from keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DEEP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\DEEP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "define some variables such as the saved model and \n",
    "information on the camera for Open-CV.Each of the variables \n",
    "set here are grouped into one of four categories. The first \n",
    "category pertains directly to the model. The second and third sections of the \n",
    "code define variables required to run and start Mediapipe \n",
    "and Open-CV. The final category is used primarily to \n",
    "analyze the frame when detected, and create the dictionary \n",
    "used in the cross-referencing of the data provided by the \n",
    "image model.\n",
    "'''\n",
    "model = load_model('smnist.h5')\n",
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "_, frame = cap.read()\n",
    "h, w, c = frame.shape\n",
    "img_counter = 0\n",
    "analysisframe = ''\n",
    "letterpred = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the program takes the input from your camera and uses our imported image processing library to display the input from the device to the computer. This portion of code focuses on getting general information from your camera and simply showing it back in a new window. However, using the Mediapipe library, we can detect the major landmarks of the hand such as the fingers and palms, and create a bounding box around the hand.  \n",
    "  \n",
    "The idea of a bounding box is a crucial component to all forms of image classification and analysis. The box allows the model to focus directly on the portion of the image needed for the function. Without this, the algorithm finds patterns in wrong places and can cause an incorrect result.  \n",
    "  \n",
    "Now , we make use of the image reshaping feature from Open-CV to resize the image to the dimensions of the bounding box, rather than creating a visual object around it. Along with this, we also use NumPy and Open-CV to modify the image to be the same dimensions as the images the model was trained on.  \n",
    "  \n",
    "Towards the top of the code, you may notice the odd sequence of variables being defined. This is due to the nature of the camera library syntax. When an image is processed and changed by Open-CV, the changes are made on top of the frame used, essentially saving the changes made to the image. The definition of multiple variables of equal value makes it so that the frame displayed in by the function is separate from the picture on which the model is being ran on.  \n",
    "  \n",
    "Finally, we need to run the trained model on the processed image and process the information output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 517ms/step\n",
      "Predicted Character 2:  H\n",
      "Confidence 2:  6.596560776233673\n",
      "Predicted Character 1:  P\n",
      "Confidence 1:  91.59116744995117\n",
      "Predicted Character 3:  Y\n",
      "Confidence 3:  1.718909665942192\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted Character 3:  N\n",
      "Confidence 3:  17.279380559921265\n",
      "Predicted Character 1:  P\n",
      "Confidence 1:  45.84163725376129\n",
      "Predicted Character 2:  T\n",
      "Confidence 2:  23.513858020305634\n",
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, frame = cap.read()\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        analysisframe = frame\n",
    "        showframe = analysisframe\n",
    "        cv2.imshow(\"Frame\", showframe)\n",
    "        framergbanalysis = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2RGB)\n",
    "        resultanalysis = hands.process(framergbanalysis)\n",
    "        hand_landmarksanalysis = resultanalysis.multi_hand_landmarks\n",
    "        if hand_landmarksanalysis:\n",
    "            for handLMsanalysis in hand_landmarksanalysis:\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                for lmanalysis in handLMsanalysis.landmark:\n",
    "                    x, y = int(lmanalysis.x * w), int(lmanalysis.y * h)\n",
    "                    if x > x_max:\n",
    "                        x_max = x\n",
    "                    if x < x_min:\n",
    "                        x_min = x\n",
    "                    if y > y_max:\n",
    "                        y_max = y\n",
    "                    if y < y_min:\n",
    "                        y_min = y\n",
    "                y_min -= 20\n",
    "                y_max += 20\n",
    "                x_min -= 20\n",
    "                x_max += 20 \n",
    "\n",
    "        analysisframe = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2GRAY)\n",
    "        analysisframe = analysisframe[y_min:y_max, x_min:x_max]\n",
    "        analysisframe = cv2.resize(analysisframe,(28,28))\n",
    "\n",
    "\n",
    "        nlist = []\n",
    "        rows,cols = analysisframe.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                k = analysisframe[i,j]\n",
    "                nlist.append(k)\n",
    "        \n",
    "        datan = pd.DataFrame(nlist).T\n",
    "        colname = []\n",
    "        for val in range(784):\n",
    "            colname.append(val)\n",
    "        datan.columns = colname\n",
    "\n",
    "        pixeldata = datan.values\n",
    "        pixeldata = pixeldata / 255\n",
    "        pixeldata = pixeldata.reshape(-1,28,28,1)\n",
    "\n",
    "        '''\n",
    "        The first two lines draw the predicted probabilities that a hand image is any of the different classes from Keras. The data is presented in the form of 2 tensors, of which, the first tensor contains information on the probabilities. A tensor is essentially a collection of feature vectors, very similar to an array. The tensor produced by the model is one dimensional, allowing it to be used with the linear algebra library NumPy to parse the information into a more pythonic form.\n",
    "\n",
    "        From here, we utilize the previously created list of classes under the variable letterpred to create a dictionary, matching the values from the tensor to the keys. This allows us to match each letter’s probability with the class it corresponds to.\n",
    "\n",
    "        Following this step, we use list comprehension to order and sort the values from highest to lowest. This then allows us to take the first few items in the list and designate them the 3 letters that closest correspond to the Sign Language image shown.\n",
    "\n",
    "        Finally, we use a for loop to cycle through all of the key:value pairs in the dictionary created to match the highest values to their corresponding keys and print out the output with each letter’s probability.\n",
    "        '''\n",
    "        prediction = model.predict(pixeldata)\n",
    "        predarray = np.array(prediction[0])\n",
    "        letter_prediction_dict = {letterpred[i]: predarray[i] for i in range(len(letterpred))}\n",
    "        predarrayordered = sorted(predarray, reverse=True)\n",
    "        high1 = predarrayordered[0]\n",
    "        high2 = predarrayordered[1]\n",
    "        high3 = predarrayordered[2]\n",
    "        for key,value in letter_prediction_dict.items():\n",
    "            if value==high1:\n",
    "                print(\"Predicted Character 1: \", key)\n",
    "                print('Confidence 1: ', 100*value)\n",
    "            elif value==high2:\n",
    "                print(\"Predicted Character 2: \", key)\n",
    "                print('Confidence 2: ', 100*value)\n",
    "            elif value==high3:\n",
    "                print(\"Predicted Character 3: \", key)\n",
    "                print('Confidence 3: ', 100*value)\n",
    "        time.sleep(5)\n",
    "\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "    if hand_landmarks:\n",
    "        for handLMs in hand_landmarks:\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            y_min -= 20\n",
    "            y_max += 20\n",
    "            x_min -= 20\n",
    "            x_max += 20\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
