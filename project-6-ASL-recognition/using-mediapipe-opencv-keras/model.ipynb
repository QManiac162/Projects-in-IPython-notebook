{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "del train_df['label']\n",
    "del test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code imports the required Keras commands and uses the ImageDataGenerator function to shape and size the images in the training data to suit the VGG19 model. The parameters in the train_datagen variable reshape the images in the training dataset so that the model understands the input image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train = label_binarizer.fit_transform(y_train)\n",
    "y_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "x_train = train_df.values\n",
    "x_test = test_df.values\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center = False,             # set input mean to 0 over the dataset\n",
    "        samplewise_center = False,              # set each sample mean to 0\n",
    "        featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization = False,   # divide each input by its std\n",
    "        zca_whitening = False,                  # apply ZCA whitening\n",
    "        rotation_range = 10,                    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1,                       # Randomly zoom image \n",
    "        width_shift_range = 0.1,                # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range = 0.1,               # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = False,                # randomly flip images\n",
    "        vertical_flip = False)                  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing the images, the model must be set to recognize all of the classes of information being used in the data, namely the 27 different groups of images. The initialization of the algorithm with the adding of variables such as the vgg19 model condenses it to 27 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 512 , activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 24 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss functions and metrics along with fitting the model to the data will create our Sign Language Recognition system. It is important to recognize the model.save() command at the end of the statement due to the length of time required to build the model. Re-training the model for every use can take hours of time.  \n",
    "  \n",
    "Line 1:  \n",
    "The model.compile() function takes many parameters, of which three are displayed in the code. The optimizer and loss parameters work together along with the epoch statement in the next line to efficiently reduce the amount of error in the model by incrementally changing computation methods on the data.  \n",
    "Along with this, the metric of choice to be optimized is the accuracy functions, which ensures that the model will have the maximum accuracy achievable after the set number of epochs.  \n",
    "  \n",
    "Line 2-3:  \n",
    "The function run here fits the previously designed model to the data from the generators developed in the first bit of code. It also defines the number of epochs or iterations the model has to enhance the accuracy of the image detection.  \n",
    "  \n",
    "Line 4:  \n",
    "Of all of the statements in the code bit, the model.save() function may be the most important part of this code, as it can potentially save hours of time when implementing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 28, 28, 75)        300       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 75)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 14, 14, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 7, 7, 50)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 25)          11275     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 7, 7, 25)          100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 25)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264049 (1.01 MB)\n",
      "Trainable params: 263749 (1.01 MB)\n",
      "Non-trainable params: 300 (1.17 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "215/215 [==============================] - 24s 106ms/step - loss: 1.0801 - accuracy: 0.6641 - val_loss: 3.5694 - val_accuracy: 0.1435\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 23s 107ms/step - loss: 0.2309 - accuracy: 0.9239 - val_loss: 1.3655 - val_accuracy: 0.5534\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 23s 108ms/step - loss: 0.1054 - accuracy: 0.9658 - val_loss: 0.1681 - val_accuracy: 0.9420\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.0650 - accuracy: 0.9800 - val_loss: 0.0762 - val_accuracy: 0.9789\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 21s 96ms/step - loss: 0.0517 - accuracy: 0.9837 - val_loss: 0.0567 - val_accuracy: 0.9796\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 21s 96ms/step - loss: 0.0339 - accuracy: 0.9898 - val_loss: 0.0436 - val_accuracy: 0.9809\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.0319 - accuracy: 0.9901 - val_loss: 0.0189 - val_accuracy: 0.9958\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.0302 - accuracy: 0.9899 - val_loss: 0.0398 - val_accuracy: 0.9838\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 21s 96ms/step - loss: 0.0181 - accuracy: 0.9942 - val_loss: 0.9734 - val_accuracy: 0.7769\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.0257 - accuracy: 0.9914 - val_loss: 0.0489 - val_accuracy: 0.9841\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 21s 96ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.0191 - val_accuracy: 0.9940\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 21s 97ms/step - loss: 0.0217 - accuracy: 0.9933 - val_loss: 0.0098 - val_accuracy: 0.9975\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 23s 108ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.0039 - val_accuracy: 0.9990\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 23s 105ms/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.0303 - val_accuracy: 0.9879\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 23s 105ms/step - loss: 0.0170 - accuracy: 0.9942 - val_loss: 0.5812 - val_accuracy: 0.8650\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 23s 107ms/step - loss: 0.0162 - accuracy: 0.9946 - val_loss: 0.0211 - val_accuracy: 0.9929\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 23s 105ms/step - loss: 0.0211 - accuracy: 0.9930 - val_loss: 0.2129 - val_accuracy: 0.9327\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 23s 105ms/step - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.1034 - val_accuracy: 0.9688\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 23s 106ms/step - loss: 0.0176 - accuracy: 0.9938 - val_loss: 0.0887 - val_accuracy: 0.9750\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 23s 107ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.0022 - val_accuracy: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DEEP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(datagen.flow(x_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (x_test, y_test))\n",
    "model.save('smnist.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
